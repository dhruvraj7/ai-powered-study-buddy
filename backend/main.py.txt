# AI Study Buddy Backend Server (FastAPI Placeholder)
# This file demonstrates the architectural design of the API layer
# as described in the Capstone Project presentation.

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any

# Initialize the FastAPI app
app = FastAPI(
    title="AI Study Buddy API",
    description="Provides endpoints for AI content generation (quizzes, flashcards)."
)

# --- Configuration ---
# Allow the frontend (running on a different port/domain) to communicate with this backend.
# In a real deployment, replace "*" with your specific frontend domain.
origins = [
    "*", # Temporary for development
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Data Models (Input and Output Schema) ---

class NoteInput(BaseModel):
    """Schema for the notes received from the frontend."""
    notes: str
    user_id: str

class QuizOutput(BaseModel):
    """Placeholder schema for the AI-generated structured data."""
    quiz: List[Dict[str, Any]]
    flashcards: List[Dict[str, Any]]

# --- API Endpoints ---

@app.get("/")
def read_root():
    """Simple health check endpoint."""
    return {"status": "ok", "service": "AI Study Buddy Backend"}

@app.post("/generate_content/", response_model=QuizOutput)
async def generate_content(input: NoteInput):
    """
    Endpoint to trigger AI content generation. 
    
    In a fully implemented system, this function would:
    1. Validate the input.
    2. Securely call the Gemini API using the server's API key.
    3. Return the structured JSON output (QuizOutput) back to the frontend.
    """
    if not input.notes:
        raise HTTPException(status_code=400, detail="Notes content cannot be empty.")
    
    # --- Placeholder Logic (Simulates AI response delay and structure) ---
    
    # Simulate a successful call to the LLM
    mock_quiz_response = QuizOutput(
        quiz=[
            {"question": "Mock Q1: What is the main topic?", "options": ["A", "B", "C"], "correctAnswer": "A"},
            {"question": "Mock Q2: What is the key concept?", "options": ["X", "Y", "Z"], "correctAnswer": "X"}
        ],
        flashcards=[
            {"term": "Mock Term 1", "definition": "Mock Definition 1"},
            {"term": "Mock Term 2", "definition": "Mock Definition 2"}
        ]
    )

    return mock_quiz_response
